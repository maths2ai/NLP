{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Prediction\n",
    "\n",
    "This Notebook tries to use an LSTM to learn the style of Tolstoj's Anna Karenina.\n",
    "If we have a string like 'abcd' the training is performed using as label at step 0 the letter 'b', while as prediction it is used the output of the letter 'a' through the neural network.\n",
    "Similiarly, at step 1, we use as a label 'c' and as prediction 'b'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing section\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text)) # Creates a set of characters from the text and transforms it into tuples\n",
    "dict_integer_char = dict(enumerate(chars))\n",
    "dict_char_integer = {char: integer for integer, char in dict_integer_char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([dict_char_integer[char] for char in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    #print(arr.shape)\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype = np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batches of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_lenght):\n",
    "    \n",
    "    char_in_batch = batch_size*seq_lenght\n",
    "    num_batches = len(arr)//char_in_batch\n",
    "    \n",
    "    # Discharging charachters that unpair the batches\n",
    "    final_arr = arr[:num_batches*char_in_batch]\n",
    "    final_arr = final_arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # Crating Labels (which in our case are the next characters)\n",
    "    \n",
    "    for i in range(0, final_arr.shape[1], seq_lenght):\n",
    "        x = final_arr[:, i : i + seq_lenght]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, : -1], y[:, -1] = x[:, 1:], final_arr[:, n +seq_lenght]\n",
    "        except :\n",
    "            y[:, : -1], y[:, -1] = x[:, 1:], final_arr[:, 0]\n",
    "        yield x, y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden = 256, n_layers = 2, drop = 0.5, lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.drop = drop\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {char: integer for integer, char in self.int2char.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), self.n_hidden, self.n_layers, dropout = self.drop, batch_first = True)\n",
    "        self.dropout = nn.Dropout(self.drop)\n",
    "        self.linear = nn.Linear(self.n_hidden, len(self.chars)) # Notice the input is equal to the hidden_dim. The short term memory is equal to the output in LSTMs\n",
    "        \n",
    "    def forward(self, char, hidden):\n",
    "        output, hidden = self.lstm(char, hidden)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = output.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data ## Why next?\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device), \n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, device, epochs = 10, batch_size = 10, seq_length = 50, lr = 0.001, clip = 5, val_frac = 0.1, print_every = 10):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    validation_index = int(len(data)*(1.0 - val_frac))\n",
    "    data, val_data = data[:validation_index], data[validation_index:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Initizialization of the firs hidden states\n",
    "        h = net.init_hidden(batch_size, device)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            # Make Tensors\n",
    "            inputs = torch.from_numpy(x).to(device)\n",
    "            labels = torch.from_numpy(y).to(device)\n",
    "\n",
    "            # Deatching h from computation\n",
    "\n",
    "            h = tuple([hidden.data for hidden in h])\n",
    "\n",
    "            # zero accumulated gradient\n",
    "            #optimizer.zero_grad()\n",
    "            net.zero_grad()\n",
    "\n",
    "            output, h = net(inputs, h)\n",
    "            #print(output.size())\n",
    "            #print(labels.view(batch_size*seq_length).size())\n",
    "            loss = criterion(output, labels.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size, device)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "\n",
    "                    val_h = tuple([hidden.data for hidden in val_h])\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, labels.view(batch_size*seq_length))\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "\n",
    "                print('Epoch {}/{}'.format(epoch+1, epochs),\n",
    "                     'Step {}'.format(counter),\n",
    "                     'Loss {:.4f}'.format(loss.item()),\n",
    "                     'Val Loss {:.4f}'.format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters/ Instatiating the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "net = RNN(chars, n_hidden, n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-85-7f1cc87688c6>:41: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Step 10 Loss 3.2409 Val Loss 3.2064\n",
      "Epoch 1/20 Step 20 Loss 3.1413 Val Loss 3.1055\n",
      "Epoch 1/20 Step 30 Loss 3.1421 Val Loss 3.1119\n",
      "Epoch 1/20 Step 40 Loss 3.1187 Val Loss 3.0910\n",
      "Epoch 1/20 Step 50 Loss 3.1444 Val Loss 3.1240\n",
      "Epoch 1/20 Step 60 Loss 3.1237 Val Loss 3.1021\n",
      "Epoch 1/20 Step 70 Loss 3.1069 Val Loss 3.0928\n",
      "Epoch 1/20 Step 80 Loss 3.1224 Val Loss 3.1082\n",
      "Epoch 1/20 Step 90 Loss 3.1179 Val Loss 3.1028\n",
      "Epoch 1/20 Step 100 Loss 3.0857 Val Loss 3.0712\n",
      "Epoch 1/20 Step 110 Loss 3.0549 Val Loss 3.0285\n",
      "Epoch 1/20 Step 120 Loss 2.9515 Val Loss 2.9178\n",
      "Epoch 1/20 Step 130 Loss 2.8645 Val Loss 2.8085\n",
      "Epoch 2/20 Step 140 Loss 2.7360 Val Loss 2.6635\n",
      "Epoch 2/20 Step 150 Loss 2.6413 Val Loss 2.5955\n",
      "Epoch 2/20 Step 160 Loss 2.5616 Val Loss 2.5219\n",
      "Epoch 2/20 Step 170 Loss 2.4910 Val Loss 2.4551\n",
      "Epoch 2/20 Step 180 Loss 2.4616 Val Loss 2.4212\n",
      "Epoch 2/20 Step 190 Loss 2.4296 Val Loss 2.3898\n",
      "Epoch 2/20 Step 200 Loss 2.4069 Val Loss 2.3631\n",
      "Epoch 2/20 Step 210 Loss 2.3724 Val Loss 2.3257\n",
      "Epoch 2/20 Step 220 Loss 2.3326 Val Loss 2.2855\n",
      "Epoch 2/20 Step 230 Loss 2.3174 Val Loss 2.2816\n",
      "Epoch 2/20 Step 240 Loss 2.2960 Val Loss 2.2492\n",
      "Epoch 2/20 Step 250 Loss 2.2371 Val Loss 2.1851\n",
      "Epoch 2/20 Step 260 Loss 2.1959 Val Loss 2.1456\n",
      "Epoch 2/20 Step 270 Loss 2.2019 Val Loss 2.1560\n",
      "Epoch 3/20 Step 280 Loss 2.1965 Val Loss 2.1454\n",
      "Epoch 3/20 Step 290 Loss 2.1660 Val Loss 2.1066\n",
      "Epoch 3/20 Step 300 Loss 2.1419 Val Loss 2.0834\n",
      "Epoch 3/20 Step 310 Loss 2.1166 Val Loss 2.0525\n",
      "Epoch 3/20 Step 320 Loss 2.0823 Val Loss 2.0195\n",
      "Epoch 3/20 Step 330 Loss 2.0472 Val Loss 1.9850\n",
      "Epoch 3/20 Step 340 Loss 2.0653 Val Loss 2.0094\n",
      "Epoch 3/20 Step 350 Loss 2.0473 Val Loss 1.9841\n",
      "Epoch 3/20 Step 360 Loss 1.9836 Val Loss 1.9267\n",
      "Epoch 3/20 Step 370 Loss 2.0163 Val Loss 1.9549\n",
      "Epoch 3/20 Step 380 Loss 1.9900 Val Loss 1.9292\n",
      "Epoch 3/20 Step 390 Loss 1.9607 Val Loss 1.9011\n",
      "Epoch 3/20 Step 400 Loss 1.9373 Val Loss 1.8604\n",
      "Epoch 3/20 Step 410 Loss 1.9421 Val Loss 1.8825\n",
      "Epoch 4/20 Step 420 Loss 1.9352 Val Loss 1.8678\n",
      "Epoch 4/20 Step 430 Loss 1.9338 Val Loss 1.8571\n",
      "Epoch 4/20 Step 440 Loss 1.9114 Val Loss 1.8438\n",
      "Epoch 4/20 Step 450 Loss 1.8490 Val Loss 1.7786\n",
      "Epoch 4/20 Step 460 Loss 1.8452 Val Loss 1.7725\n",
      "Epoch 4/20 Step 470 Loss 1.8792 Val Loss 1.8084\n",
      "Epoch 4/20 Step 480 Loss 1.8623 Val Loss 1.7835\n",
      "Epoch 4/20 Step 490 Loss 1.8612 Val Loss 1.7891\n",
      "Epoch 4/20 Step 500 Loss 1.8478 Val Loss 1.7850\n",
      "Epoch 4/20 Step 510 Loss 1.8321 Val Loss 1.7639\n",
      "Epoch 4/20 Step 520 Loss 1.8494 Val Loss 1.7786\n",
      "Epoch 4/20 Step 530 Loss 1.8029 Val Loss 1.7384\n",
      "Epoch 4/20 Step 540 Loss 1.7606 Val Loss 1.6907\n",
      "Epoch 4/20 Step 550 Loss 1.8149 Val Loss 1.7511\n",
      "Epoch 5/20 Step 560 Loss 1.7848 Val Loss 1.7088\n",
      "Epoch 5/20 Step 570 Loss 1.7679 Val Loss 1.6997\n",
      "Epoch 5/20 Step 580 Loss 1.7432 Val Loss 1.6791\n",
      "Epoch 5/20 Step 590 Loss 1.7527 Val Loss 1.6793\n",
      "Epoch 5/20 Step 600 Loss 1.7355 Val Loss 1.6729\n",
      "Epoch 5/20 Step 610 Loss 1.7296 Val Loss 1.6637\n",
      "Epoch 5/20 Step 620 Loss 1.7256 Val Loss 1.6570\n",
      "Epoch 5/20 Step 630 Loss 1.7378 Val Loss 1.6685\n",
      "Epoch 5/20 Step 640 Loss 1.7130 Val Loss 1.6462\n",
      "Epoch 5/20 Step 650 Loss 1.7033 Val Loss 1.6355\n",
      "Epoch 5/20 Step 660 Loss 1.6871 Val Loss 1.6211\n",
      "Epoch 5/20 Step 670 Loss 1.7076 Val Loss 1.6401\n",
      "Epoch 5/20 Step 680 Loss 1.7091 Val Loss 1.6381\n",
      "Epoch 5/20 Step 690 Loss 1.6821 Val Loss 1.6157\n",
      "Epoch 6/20 Step 700 Loss 1.6844 Val Loss 1.6170\n",
      "Epoch 6/20 Step 710 Loss 1.6739 Val Loss 1.6147\n",
      "Epoch 6/20 Step 720 Loss 1.6579 Val Loss 1.5877\n",
      "Epoch 6/20 Step 730 Loss 1.6788 Val Loss 1.6109\n",
      "Epoch 6/20 Step 740 Loss 1.6493 Val Loss 1.5857\n",
      "Epoch 6/20 Step 750 Loss 1.6227 Val Loss 1.5601\n",
      "Epoch 6/20 Step 760 Loss 1.6619 Val Loss 1.6009\n",
      "Epoch 6/20 Step 770 Loss 1.6580 Val Loss 1.5806\n",
      "Epoch 6/20 Step 780 Loss 1.6319 Val Loss 1.5659\n",
      "Epoch 6/20 Step 790 Loss 1.6217 Val Loss 1.5559\n",
      "Epoch 6/20 Step 800 Loss 1.6368 Val Loss 1.5698\n",
      "Epoch 6/20 Step 810 Loss 1.6187 Val Loss 1.5584\n",
      "Epoch 6/20 Step 820 Loss 1.5864 Val Loss 1.5273\n",
      "Epoch 6/20 Step 830 Loss 1.6378 Val Loss 1.5637\n",
      "Epoch 7/20 Step 840 Loss 1.5908 Val Loss 1.5181\n",
      "Epoch 7/20 Step 850 Loss 1.5997 Val Loss 1.5388\n",
      "Epoch 7/20 Step 860 Loss 1.5811 Val Loss 1.5176\n",
      "Epoch 7/20 Step 870 Loss 1.6037 Val Loss 1.5363\n",
      "Epoch 7/20 Step 880 Loss 1.5898 Val Loss 1.5467\n",
      "Epoch 7/20 Step 890 Loss 1.5909 Val Loss 1.5195\n",
      "Epoch 7/20 Step 900 Loss 1.5803 Val Loss 1.5313\n",
      "Epoch 7/20 Step 910 Loss 1.5555 Val Loss 1.4972\n",
      "Epoch 7/20 Step 920 Loss 1.5716 Val Loss 1.5121\n",
      "Epoch 7/20 Step 930 Loss 1.5711 Val Loss 1.5061\n",
      "Epoch 7/20 Step 940 Loss 1.5682 Val Loss 1.5005\n",
      "Epoch 7/20 Step 950 Loss 1.5823 Val Loss 1.5183\n",
      "Epoch 7/20 Step 960 Loss 1.5725 Val Loss 1.5193\n",
      "Epoch 7/20 Step 970 Loss 1.5814 Val Loss 1.5356\n",
      "Epoch 8/20 Step 980 Loss 1.5560 Val Loss 1.4930\n",
      "Epoch 8/20 Step 990 Loss 1.5585 Val Loss 1.4945\n",
      "Epoch 8/20 Step 1000 Loss 1.5456 Val Loss 1.4919\n",
      "Epoch 8/20 Step 1010 Loss 1.5836 Val Loss 1.5245\n",
      "Epoch 8/20 Step 1020 Loss 1.5468 Val Loss 1.4948\n",
      "Epoch 8/20 Step 1030 Loss 1.5339 Val Loss 1.4731\n",
      "Epoch 8/20 Step 1040 Loss 1.5482 Val Loss 1.4926\n",
      "Epoch 8/20 Step 1050 Loss 1.5340 Val Loss 1.4788\n",
      "Epoch 8/20 Step 1060 Loss 1.5346 Val Loss 1.4731\n",
      "Epoch 8/20 Step 1070 Loss 1.5271 Val Loss 1.4708\n",
      "Epoch 8/20 Step 1080 Loss 1.5322 Val Loss 1.4717\n",
      "Epoch 8/20 Step 1090 Loss 1.5133 Val Loss 1.4601\n",
      "Epoch 8/20 Step 1100 Loss 1.5074 Val Loss 1.4615\n",
      "Epoch 8/20 Step 1110 Loss 1.5178 Val Loss 1.4694\n",
      "Epoch 9/20 Step 1120 Loss 1.5370 Val Loss 1.4780\n",
      "Epoch 9/20 Step 1130 Loss 1.5294 Val Loss 1.4645\n",
      "Epoch 9/20 Step 1140 Loss 1.5257 Val Loss 1.4678\n",
      "Epoch 9/20 Step 1150 Loss 1.5372 Val Loss 1.4753\n",
      "Epoch 9/20 Step 1160 Loss 1.4909 Val Loss 1.4484\n",
      "Epoch 9/20 Step 1170 Loss 1.5035 Val Loss 1.4556\n",
      "Epoch 9/20 Step 1180 Loss 1.4919 Val Loss 1.4390\n",
      "Epoch 9/20 Step 1190 Loss 1.5241 Val Loss 1.4764\n",
      "Epoch 9/20 Step 1200 Loss 1.4687 Val Loss 1.4139\n",
      "Epoch 9/20 Step 1210 Loss 1.4880 Val Loss 1.4242\n",
      "Epoch 9/20 Step 1220 Loss 1.4819 Val Loss 1.4425\n",
      "Epoch 9/20 Step 1230 Loss 1.4622 Val Loss 1.4218\n",
      "Epoch 9/20 Step 1240 Loss 1.4761 Val Loss 1.4316\n",
      "Epoch 9/20 Step 1250 Loss 1.4874 Val Loss 1.4364\n",
      "Epoch 10/20 Step 1260 Loss 1.4828 Val Loss 1.4347\n",
      "Epoch 10/20 Step 1270 Loss 1.4752 Val Loss 1.4171\n",
      "Epoch 10/20 Step 1280 Loss 1.4971 Val Loss 1.4352\n",
      "Epoch 10/20 Step 1290 Loss 1.4777 Val Loss 1.4276\n",
      "Epoch 10/20 Step 1300 Loss 1.4625 Val Loss 1.4151\n",
      "Epoch 10/20 Step 1310 Loss 1.4825 Val Loss 1.4267\n",
      "Epoch 10/20 Step 1320 Loss 1.4546 Val Loss 1.4015\n",
      "Epoch 10/20 Step 1330 Loss 1.4490 Val Loss 1.3920\n",
      "Epoch 10/20 Step 1340 Loss 1.4477 Val Loss 1.3970\n",
      "Epoch 10/20 Step 1350 Loss 1.4332 Val Loss 1.3938\n",
      "Epoch 10/20 Step 1360 Loss 1.4440 Val Loss 1.3903\n",
      "Epoch 10/20 Step 1370 Loss 1.4249 Val Loss 1.3856\n",
      "Epoch 10/20 Step 1380 Loss 1.4538 Val Loss 1.4068\n",
      "Epoch 10/20 Step 1390 Loss 1.4325 Val Loss 1.3826\n",
      "Epoch 11/20 Step 1400 Loss 1.4735 Val Loss 1.4220\n",
      "Epoch 11/20 Step 1410 Loss 1.4831 Val Loss 1.4310\n",
      "Epoch 11/20 Step 1420 Loss 1.4683 Val Loss 1.4203\n",
      "Epoch 11/20 Step 1430 Loss 1.4537 Val Loss 1.4042\n",
      "Epoch 11/20 Step 1440 Loss 1.4636 Val Loss 1.4141\n",
      "Epoch 11/20 Step 1450 Loss 1.4045 Val Loss 1.3541\n",
      "Epoch 11/20 Step 1460 Loss 1.4288 Val Loss 1.3897\n",
      "Epoch 11/20 Step 1470 Loss 1.4167 Val Loss 1.3790\n",
      "Epoch 11/20 Step 1480 Loss 1.4504 Val Loss 1.4003\n",
      "Epoch 11/20 Step 1490 Loss 1.4267 Val Loss 1.3838\n",
      "Epoch 11/20 Step 1500 Loss 1.4238 Val Loss 1.3734\n",
      "Epoch 11/20 Step 1510 Loss 1.4031 Val Loss 1.3658\n",
      "Epoch 11/20 Step 1520 Loss 1.4378 Val Loss 1.3893\n",
      "Epoch 12/20 Step 1530 Loss 1.5001 Val Loss 1.3862\n",
      "Epoch 12/20 Step 1540 Loss 1.4464 Val Loss 1.3949\n",
      "Epoch 12/20 Step 1550 Loss 1.4425 Val Loss 1.3911\n",
      "Epoch 12/20 Step 1560 Loss 1.4538 Val Loss 1.4069\n",
      "Epoch 12/20 Step 1570 Loss 1.4082 Val Loss 1.3608\n",
      "Epoch 12/20 Step 1580 Loss 1.3878 Val Loss 1.3414\n",
      "Epoch 12/20 Step 1590 Loss 1.3813 Val Loss 1.3385\n",
      "Epoch 12/20 Step 1600 Loss 1.4035 Val Loss 1.3555\n",
      "Epoch 12/20 Step 1610 Loss 1.4011 Val Loss 1.3668\n",
      "Epoch 12/20 Step 1620 Loss 1.3919 Val Loss 1.3610\n",
      "Epoch 12/20 Step 1630 Loss 1.4290 Val Loss 1.3870\n",
      "Epoch 12/20 Step 1640 Loss 1.4055 Val Loss 1.3640\n",
      "Epoch 12/20 Step 1650 Loss 1.3673 Val Loss 1.3322\n",
      "Epoch 12/20 Step 1660 Loss 1.4350 Val Loss 1.3982\n",
      "Epoch 13/20 Step 1670 Loss 1.4078 Val Loss 1.3580\n",
      "Epoch 13/20 Step 1680 Loss 1.4113 Val Loss 1.3657\n",
      "Epoch 13/20 Step 1690 Loss 1.3907 Val Loss 1.3533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Step 1700 Loss 1.3917 Val Loss 1.3502\n",
      "Epoch 13/20 Step 1710 Loss 1.3752 Val Loss 1.3322\n",
      "Epoch 13/20 Step 1720 Loss 1.3769 Val Loss 1.3306\n",
      "Epoch 13/20 Step 1730 Loss 1.4131 Val Loss 1.3723\n",
      "Epoch 13/20 Step 1740 Loss 1.3904 Val Loss 1.3547\n",
      "Epoch 13/20 Step 1750 Loss 1.3505 Val Loss 1.3289\n",
      "Epoch 13/20 Step 1760 Loss 1.3826 Val Loss 1.3481\n",
      "Epoch 13/20 Step 1770 Loss 1.4036 Val Loss 1.3578\n",
      "Epoch 13/20 Step 1780 Loss 1.3638 Val Loss 1.3402\n",
      "Epoch 13/20 Step 1790 Loss 1.3649 Val Loss 1.3290\n",
      "Epoch 13/20 Step 1800 Loss 1.3872 Val Loss 1.3508\n",
      "Epoch 14/20 Step 1810 Loss 1.4018 Val Loss 1.3588\n",
      "Epoch 14/20 Step 1820 Loss 1.3742 Val Loss 1.3442\n",
      "Epoch 14/20 Step 1830 Loss 1.3821 Val Loss 1.3544\n",
      "Epoch 14/20 Step 1840 Loss 1.3420 Val Loss 1.3069\n",
      "Epoch 14/20 Step 1850 Loss 1.3313 Val Loss 1.2948\n",
      "Epoch 14/20 Step 1860 Loss 1.3827 Val Loss 1.3498\n",
      "Epoch 14/20 Step 1870 Loss 1.3887 Val Loss 1.3488\n",
      "Epoch 14/20 Step 1880 Loss 1.3715 Val Loss 1.3349\n",
      "Epoch 14/20 Step 1890 Loss 1.4070 Val Loss 1.3575\n",
      "Epoch 14/20 Step 1900 Loss 1.3800 Val Loss 1.3432\n",
      "Epoch 14/20 Step 1910 Loss 1.3710 Val Loss 1.3500\n",
      "Epoch 14/20 Step 1920 Loss 1.3764 Val Loss 1.3426\n",
      "Epoch 14/20 Step 1930 Loss 1.3343 Val Loss 1.2944\n",
      "Epoch 14/20 Step 1940 Loss 1.3940 Val Loss 1.3675\n",
      "Epoch 15/20 Step 1950 Loss 1.3663 Val Loss 1.3228\n",
      "Epoch 15/20 Step 1960 Loss 1.3704 Val Loss 1.3291\n",
      "Epoch 15/20 Step 1970 Loss 1.3614 Val Loss 1.3183\n",
      "Epoch 15/20 Step 1980 Loss 1.3530 Val Loss 1.3136\n",
      "Epoch 15/20 Step 1990 Loss 1.3449 Val Loss 1.3197\n",
      "Epoch 15/20 Step 2000 Loss 1.3310 Val Loss 1.3129\n",
      "Epoch 15/20 Step 2010 Loss 1.3514 Val Loss 1.3178\n",
      "Epoch 15/20 Step 2020 Loss 1.3717 Val Loss 1.3269\n",
      "Epoch 15/20 Step 2030 Loss 1.3385 Val Loss 1.3140\n",
      "Epoch 15/20 Step 2040 Loss 1.3631 Val Loss 1.3298\n",
      "Epoch 15/20 Step 2050 Loss 1.3545 Val Loss 1.3142\n",
      "Epoch 15/20 Step 2060 Loss 1.3515 Val Loss 1.3245\n",
      "Epoch 15/20 Step 2070 Loss 1.3675 Val Loss 1.3229\n",
      "Epoch 15/20 Step 2080 Loss 1.3505 Val Loss 1.3136\n",
      "Epoch 16/20 Step 2090 Loss 1.3583 Val Loss 1.3175\n",
      "Epoch 16/20 Step 2100 Loss 1.3506 Val Loss 1.3101\n",
      "Epoch 16/20 Step 2110 Loss 1.3358 Val Loss 1.2962\n",
      "Epoch 16/20 Step 2120 Loss 1.3531 Val Loss 1.3181\n",
      "Epoch 16/20 Step 2130 Loss 1.3163 Val Loss 1.2997\n",
      "Epoch 16/20 Step 2140 Loss 1.3321 Val Loss 1.2930\n",
      "Epoch 16/20 Step 2150 Loss 1.3600 Val Loss 1.3345\n",
      "Epoch 16/20 Step 2160 Loss 1.3366 Val Loss 1.3021\n",
      "Epoch 16/20 Step 2170 Loss 1.3341 Val Loss 1.3047\n",
      "Epoch 16/20 Step 2180 Loss 1.3251 Val Loss 1.3029\n",
      "Epoch 16/20 Step 2190 Loss 1.3549 Val Loss 1.3204\n",
      "Epoch 16/20 Step 2200 Loss 1.3267 Val Loss 1.3062\n",
      "Epoch 16/20 Step 2210 Loss 1.2966 Val Loss 1.2714\n",
      "Epoch 16/20 Step 2220 Loss 1.3400 Val Loss 1.3072\n",
      "Epoch 17/20 Step 2230 Loss 1.3174 Val Loss 1.2847\n",
      "Epoch 17/20 Step 2240 Loss 1.3251 Val Loss 1.3039\n",
      "Epoch 17/20 Step 2250 Loss 1.3132 Val Loss 1.2830\n",
      "Epoch 17/20 Step 2260 Loss 1.3326 Val Loss 1.2983\n",
      "Epoch 17/20 Step 2270 Loss 1.3278 Val Loss 1.3153\n",
      "Epoch 17/20 Step 2280 Loss 1.3242 Val Loss 1.2877\n",
      "Epoch 17/20 Step 2290 Loss 1.3361 Val Loss 1.3189\n",
      "Epoch 17/20 Step 2300 Loss 1.2987 Val Loss 1.2770\n",
      "Epoch 17/20 Step 2310 Loss 1.3192 Val Loss 1.2939\n",
      "Epoch 17/20 Step 2320 Loss 1.3130 Val Loss 1.2930\n",
      "Epoch 17/20 Step 2330 Loss 1.3112 Val Loss 1.2808\n",
      "Epoch 17/20 Step 2340 Loss 1.3372 Val Loss 1.3082\n",
      "Epoch 17/20 Step 2350 Loss 1.3267 Val Loss 1.3090\n",
      "Epoch 17/20 Step 2360 Loss 1.3554 Val Loss 1.3380\n",
      "Epoch 18/20 Step 2370 Loss 1.3090 Val Loss 1.2800\n",
      "Epoch 18/20 Step 2380 Loss 1.3200 Val Loss 1.2898\n",
      "Epoch 18/20 Step 2390 Loss 1.3229 Val Loss 1.2989\n",
      "Epoch 18/20 Step 2400 Loss 1.3373 Val Loss 1.3081\n",
      "Epoch 18/20 Step 2410 Loss 1.3442 Val Loss 1.3100\n",
      "Epoch 18/20 Step 2420 Loss 1.3172 Val Loss 1.2792\n",
      "Epoch 18/20 Step 2430 Loss 1.3283 Val Loss 1.3018\n",
      "Epoch 18/20 Step 2440 Loss 1.3016 Val Loss 1.2801\n",
      "Epoch 18/20 Step 2450 Loss 1.3040 Val Loss 1.2835\n",
      "Epoch 18/20 Step 2460 Loss 1.3096 Val Loss 1.2841\n",
      "Epoch 18/20 Step 2470 Loss 1.3104 Val Loss 1.2833\n",
      "Epoch 18/20 Step 2480 Loss 1.2912 Val Loss 1.2729\n",
      "Epoch 18/20 Step 2490 Loss 1.2990 Val Loss 1.2831\n",
      "Epoch 18/20 Step 2500 Loss 1.3024 Val Loss 1.2820\n",
      "Epoch 19/20 Step 2510 Loss 1.3143 Val Loss 1.2881\n",
      "Epoch 19/20 Step 2520 Loss 1.3230 Val Loss 1.2859\n",
      "Epoch 19/20 Step 2530 Loss 1.3231 Val Loss 1.3008\n",
      "Epoch 19/20 Step 2540 Loss 1.3377 Val Loss 1.3008\n",
      "Epoch 19/20 Step 2550 Loss 1.3047 Val Loss 1.2928\n",
      "Epoch 19/20 Step 2560 Loss 1.3052 Val Loss 1.2878\n",
      "Epoch 19/20 Step 2570 Loss 1.2957 Val Loss 1.2671\n",
      "Epoch 19/20 Step 2580 Loss 1.3327 Val Loss 1.3083\n",
      "Epoch 19/20 Step 2590 Loss 1.2796 Val Loss 1.2579\n",
      "Epoch 19/20 Step 2600 Loss 1.2944 Val Loss 1.2604\n",
      "Epoch 19/20 Step 2610 Loss 1.2949 Val Loss 1.2835\n",
      "Epoch 19/20 Step 2620 Loss 1.2858 Val Loss 1.2670\n",
      "Epoch 19/20 Step 2630 Loss 1.2837 Val Loss 1.2718\n",
      "Epoch 19/20 Step 2640 Loss 1.3102 Val Loss 1.2887\n",
      "Epoch 20/20 Step 2650 Loss 1.3082 Val Loss 1.2793\n",
      "Epoch 20/20 Step 2660 Loss 1.3053 Val Loss 1.2684\n",
      "Epoch 20/20 Step 2670 Loss 1.3130 Val Loss 1.2830\n",
      "Epoch 20/20 Step 2680 Loss 1.3097 Val Loss 1.2797\n",
      "Epoch 20/20 Step 2690 Loss 1.2981 Val Loss 1.2716\n",
      "Epoch 20/20 Step 2700 Loss 1.3016 Val Loss 1.2790\n",
      "Epoch 20/20 Step 2710 Loss 1.2788 Val Loss 1.2538\n",
      "Epoch 20/20 Step 2720 Loss 1.2664 Val Loss 1.2372\n",
      "Epoch 20/20 Step 2730 Loss 1.2758 Val Loss 1.2556\n",
      "Epoch 20/20 Step 2740 Loss 1.2682 Val Loss 1.2531\n",
      "Epoch 20/20 Step 2750 Loss 1.2749 Val Loss 1.2539\n",
      "Epoch 20/20 Step 2760 Loss 1.2685 Val Loss 1.2511\n",
      "Epoch 20/20 Step 2770 Loss 1.2954 Val Loss 1.2733\n",
      "Epoch 20/20 Step 2780 Loss 1.2760 Val Loss 1.2507\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded, device, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function. It is used to create an initial hidden state for the LSTM and to create our novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, device, size, first = 'The', top_k = None ):\n",
    "    \n",
    "    net.eval()\n",
    "    chars = [ch for ch in first]\n",
    "    h = net.init_hidden(1, device)\n",
    "    \n",
    "    for ch in first:\n",
    "        char, h = predict(net, ch, h, top_k = top_k)\n",
    "        \n",
    "    chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k = top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h = None, top_k = None):\n",
    "    \n",
    "    x = np.array([[net.char2int[char]]]) # Double [[]] to emulate batch_size = 1\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(x).to(device)\n",
    "    h = tuple([hidden.data for hidden in h])\n",
    "    \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # output has shape (batch*seq_length, num_chars in dictionary)\n",
    "    \n",
    "    p = F.softmax(output, dim = 1).data\n",
    "    \n",
    "    # getting predictions\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.cpu().numpy().squeeze() # De-tensorize and removing batch\n",
    "        \n",
    "    p = p.cpu().numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p = p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna was\n",
      "daying about his.\n",
      "\n",
      "\"I'll been to her. And I should go in to that,\" said Anna, trying to see\n",
      "them, and he\n",
      "does not talking to his subject of her\n",
      "face to strack his wife, was satisficulting and settled thas evening to all happened, and this thoughts\n",
      "of the\n",
      "pluthing, at this part were for in a tray of seren exact. Besides he had been there to say that it always decided to him.\n",
      "\n",
      "\"Well, thank you're always satisfied. I'm stopping on all of the sacrial thing.\"\n",
      "\n",
      "She conceined how at those were still she felt in the regarted coarse.\n",
      "\n",
      "\"And I did not come to him,\" said Anna, and his feeling said a sign, which stood stringly\n",
      "in the carriage she heard the\n",
      "room, solity at his fore hands.\n",
      "\n",
      "Stepan Arkadyevitch walked and still sore from surprise in which he felt that he had\n",
      "come for an understand were so than she had\n",
      "taken the country. He called to him. She flung town the stream, and thisteled with\n",
      "horror, and he went out of\n",
      "the\n",
      "clear house and daighter.\n",
      "\n",
      "\"A struck over on that table,\" said Anna,\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, device, 1000, first='Anna', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
